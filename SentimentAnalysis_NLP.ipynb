{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6afbad2d-38dc-487b-9c46-bd99f9905732",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "First, think about C3PO, Luke Skywalker's robot sidekick in Star Wars. C3P0 is a fantasized version of human-computer interaction in the distant future. However, humans interacting with machines is an every day reality for us. Your home or car smart assistant (Alexa), customer service on websites or phone lines, autocorrect features, etc. are all examples of Natural Language Processing.\n",
    "\n",
    "Natural Language Processing (NLP) is the field of deriving meaningful information from human speech. NLP is a branch of computer science, or more specifically a branch of artificial intelligence, concerned with allowing computers the ability to understand human speech either in a written or spoken format. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffbfcf-f28c-4557-b22d-22fb382ca9f8",
   "metadata": {},
   "source": [
    "### There are many types of NLP\n",
    "There are many different varieties of natural language processing. These are just a few real world examples of these techniques to give an idea of how this is used today.\n",
    "\n",
    "#### Sentiment Analysis\n",
    "This is most of what we will be doing today. Sentiment analysis examines text in order to identify the general \"feeling\" of the text. Take this example...\n",
    "Businesses are using sentiment analysis today to monitor and evaluate customer service. Does this customer seem satisfied?\n",
    "\n",
    "![airbnb tweet](./images/airbnb.png)\n",
    "\n",
    "This person is not happy. By analyzing sentiment analysis on customer support chats, tweets, etc. a company can get insights on where their service model is not working. \n",
    "    \n",
    "#### Topic Modeling\n",
    "Topic modeling is an unsupervised machine learning technique that is capable of scanning a set of documents, detecting patterns within them, and automatically clustering word groups or similar expressions that characterize the documents.  An example...\n",
    "Imagine that you work at a legal firm and someone at a company has embezzled money. You need to figure out who that person is and you are monitoring company emails from the last six months. There are probably thousands of emails and you don't want to waste time reading all of them. In this case, you can have a computer read the text of the emails and identify the ones that are relevant to the topic of money, narrowing down the amount of emails needed to read\n",
    "    \n",
    "![email examples](./images/topicmodeling.png)\n",
    "    \n",
    "#### Text Generation\n",
    "Text generation is simply the task of producing new text. A very common example of this is autocomplete or autofill features, such as when texting or in a search engine. Take the following example, we all use this every day right?\n",
    "\n",
    "![google autofill](./images/google.png)\n",
    "\n",
    "The code simple predicts what you might type next..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c559f-2d00-41bc-b0d7-4ec11dcfcf0c",
   "metadata": {},
   "source": [
    "## Example 1: VADER sentiment scoring\n",
    "\n",
    "VADER - Valence Aware Dictionary for sEntiment Reasoning. VADER is a model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity of emotion. This model does not account for relationships between words. This is the \"bag of word\" approach. All words in the text are thrown into a bag and scored. The cumulative score determines the final rating. More on this later.\n",
    "\n",
    "## Data Gathering\n",
    "\n",
    "The first part of most projects like this is getting data. This could be another workshop in itself so for the sake of our meeting today we will be using some sample data that I provide about product reviews on Amazon. \n",
    "\n",
    "There are many potential sources of data. It may be available via an API. You might have to grab it using a web scraper. You may be lucky and someone has already gathered it for you. You can process basically any textual data from many file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1ee8b-33bb-4ffa-a2bf-c2e07fc163f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk     #natural language toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5816cf-e838-4df8-a209-0dd90b74152a",
   "metadata": {},
   "source": [
    "### Import Sample Data from NLTK\n",
    "\n",
    "For starters, we will use some sample data that I am providing to you. This is data from Amazon.com for product reviews.\n",
    "\n",
    "There are more than 500,000 rows of data in this dataset so it is big! There are 10 columns for each review but today we will be concerned really only with the \"Score\", \"Summary\", and \"Text\" columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd32c1-9559-4902-91e8-a5b3fbff8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/ep9k/Desktop/sentiment_analysis/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6ed84-6fbd-4cbc-9a55-980c37fda353",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bae6d-b76f-4232-a436-51e53fec74bc",
   "metadata": {},
   "source": [
    "Let's see the text of just the first row of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35089cca-b397-4f99-842d-134520282718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the text of just the first review\n",
    "print(data['Text'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bce9d-c0c1-4e1c-916e-a21ad853053a",
   "metadata": {},
   "source": [
    "Here you can see the size and shape of the test data. There are over 500,000 reviews in this dataset so let's make it smaller, just for our the purposes of our workshop today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1476e-1d5f-4fe3-af79-c3ea8d7732ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbccc4a1-3f26-4190-98d1-37c6d77f1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am simplifying the dataset to just the first 500 reviews\n",
    "data = data.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6df3a-cfc8-4358-a21f-8848bf9aaabd",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "Let's play with the data to see what is in it. There are a lot of reasons for doing this, but basically EDA's main purpose is to explore the data and understand it more before making assumptions about it. This might also help you identify outliers, and find interesting relationships between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb44e9e-4d4e-46f9-90d8-5db7d54ae9c8",
   "metadata": {},
   "source": [
    "Products have a score of 1-5. This is basically a star rating. Let's see how many times each score occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b7e11-e9ad-4826-a4ab-6e1aed4170b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6cbd35-b7a4-4bb6-a472-a06a2ad54fc7",
   "metadata": {},
   "source": [
    "Let's use Matplotlib to visualize the data in a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee8bcd-4aa4-468f-a12b-49224641e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = data['Score'].value_counts().sort_index().plot(kind='bar',\n",
    "                                                    title='Count of Reviews by Stars',\n",
    "                                                    figsize=(10, 5))\n",
    "ax.set_xlabel('Review Rating')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff346762-0f55-4754-8a6a-1119ee9baf1e",
   "metadata": {},
   "source": [
    "It looks like there are a lot of 5 star reviews. I assume this means the corresponding 'text' for each review will be positive. We will test this assumption later. First we need to process the text some more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2763c4f-8317-4b24-87ca-3d7ddac0698e",
   "metadata": {},
   "source": [
    "# NLTK Basics\n",
    "\n",
    "NLTK (Natural Language Toolkit) is a python library for working with human language data. It is just one of many libraries which you can use for Natural Language Processing. A lot of the data you might be analyzing is unstructured data (aka human text). Before you can analyze data programmatically, you need to do some pre-processing. \n",
    "\n",
    "Let's start by processing the text of one review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33f67d3-44af-40ad-9589-1503d1076004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting text of one review\n",
    "example_text = data['Text'][50]\n",
    "print(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a0133-f906-4661-a9e4-4e8a7ae4821c",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking textual data into words, terms, sentences, or some other meaninful chunk as discrete elements. After data gathering and maybe some EDA, tokenization is often the next step in the NLP workflow. The effect of this process is it breaks the text into a data structure that the computer can interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc50b04c-8260-4437-aada-2fce331a480a",
   "metadata": {},
   "source": [
    "NLTK allows tokenization out of the box with word_tokenize(). However, it is a little bit messy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f0b6c-d2d3-465d-bf5c-fd8d7645a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(example_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a9f7f8-5e8e-46f2-94bb-99d39a94055d",
   "metadata": {},
   "source": [
    "## Part of Speech\n",
    "We can find the part of speech for each token. [Here](https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/) is a (complete?) list of NLTK's parts of speech. \n",
    "\n",
    "We don't really need the part of speech for our later exercises today but this is useful and you might want this in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cdee95-386c-4fc3-9dec-be86ae96f1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d19cf2-cb12-40d6-90fe-8b86e62a2049",
   "metadata": {},
   "source": [
    "### Vader Sentiment Scoring\n",
    "\n",
    "I introduced this model of sentiment analysis earlier but now for more details. The VADER model uses the \"bag of words\" approach to produce a sentiment score. This model takes all the words in your sentence/corpus and assigns a score to each word of positive, negative, neutral. Then the model takes the sum of all those scores and the result is the overall sentiment of that sentence. Stop words are removed from the scoring. Stop words are common words like \"the\", \"and\", \"a/an\" that don't contribute to the sentiment of a phrase or sentence. Keep in mind, this is a relatively simplistic way of performing sentiment analysis and does not take in to account the relationship between words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7386de-31c3-4034-8b8c-1fb5a490f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e273ff1-4e27-4b37-8beb-09416d5d6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0787f92-fcf0-4033-9e4a-e78fbc1f5862",
   "metadata": {},
   "source": [
    "A few quick examples of the Sentiment Intensity Analyzer in action. First, a couple of individual words followed by a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842de40-58dd-4651-8499-105b28d20f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"I\")   # no score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a0eff-cdaa-42ef-a958-05df21c61b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf114c0-8296-45d2-8235-a52349f96ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3449deb-935e-407d-8672-9c2527d1b9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"happy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5b4282-85bf-4d7c-9f98-59f918c89a31",
   "metadata": {},
   "source": [
    "All together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fbaa4-58af-4b47-9566-1cf17522e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('I am so happy!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec59adc-2cad-4307-a703-7d567d267070",
   "metadata": {},
   "source": [
    "Punctuation also has an impact. The same sentence without the '!' is scored less positively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed4409c-94f0-4e30-9da1-08dc6902ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"I am so happy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca2bf43-2304-4683-b6c9-ba9e438e6c00",
   "metadata": {},
   "source": [
    "The compound score has a range of values from -1 to +1 to rate how positive (+1) or negative (-1) a statement is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4e036-eae0-4873-a806-9941fd3e8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('This is the worst!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58680ce2-b603-43ff-98e0-95a1e6eaae33",
   "metadata": {},
   "source": [
    "Now let's run this on our example text from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b3cac-10f3-4f1c-839e-e580577fd501",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_text)\n",
    "sia.polarity_scores(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6e2b4-e0af-4626-961c-4112a7ee912a",
   "metadata": {},
   "source": [
    "Now let's run it on our entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a0314-d172-41df-bd24-73492dce951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a dictionary which holds the polarity score of each review\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    text = row['Text']\n",
    "    myId = row['Id']\n",
    "    results[myId] = sia.polarity_scores(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb44e3-fec8-490b-bca5-6cc1e712ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vaders = pd.DataFrame(results).T\n",
    "\n",
    "#this line sets the index column and calls it 'id'\n",
    "vaders = vaders.reset_index().rename(columns={'index':'Id'})    \n",
    "\n",
    "vaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a31dc-079a-4a39-b0ab-f152ae09a8a2",
   "metadata": {},
   "source": [
    "Now let's merge our vaders sentiment scores with our original dataframe. Now we have sentiment score and metadata added to our original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb776e7-8a36-492d-bd3c-9573ef83e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it is actually very easy to do\n",
    "data = data.merge(vaders, how='left')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0c3fc-68fd-40a6-a30f-5c208640db3f",
   "metadata": {},
   "source": [
    "### Testing Assumptions\n",
    "\n",
    "Let's now test some of our assumptions. I would assume that if a reviewer gave a product a 5 star review, then the text would have a positive sentiment. Accordingly, a one star review would have text with a negative sentiment. \n",
    "\n",
    "To start, I'll look at the sentiments of 5 star and 1 star reviews. Then we will visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0023cf-aab1-4ab2-bfe5-dc26e311cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_stars = data.loc[data['Score'] == 5]\n",
    "# limit results to just the first 10 \n",
    "five_stars = five_stars.head()\n",
    "\n",
    "five_stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdbd699-57e8-4524-89d9-301737975a0e",
   "metadata": {},
   "source": [
    "From looking at the five star results, it does indeed look like they have positive sentiment scores. In fact, most of them have a very positive sentiment score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532b44b-3253-46b5-84d6-ecd7b2e2d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_stars = data.loc[data['Score'] == 1]\n",
    "# limit results to just the first 10 \n",
    "one_stars = one_stars.head()\n",
    "\n",
    "one_stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f0b8a-b5e7-4b51-ae19-603b65494f10",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "I have already used a basic plotting library, MatPlotLib, to do a few bar plots. Now I'll use an alternative called Seaborn. Seaborn is an extension to MatPlotLib which allows more sophisticated statistical graphics. But it also looks nice for simple plots too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5fbdb-3189-49aa-a837-8f9d5b7682c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#overall compound score of each review\n",
    "ax = sns.barplot(data=data, x='Score', y='compound', ci=None)  #ci is for the confidence interval\n",
    "ax.set_title('Compound Score by Amazon Stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9035cc-2822-4ca3-bf1c-a0699b878a0a",
   "metadata": {},
   "source": [
    "Here you see a positive relationship between score and positive columns. Positivity score increases as the score increases. This means that one star reviews have less positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867cf7c-7f38-449d-add3-5467d0081dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#positive sentiment score of each review\n",
    "ax = sns.barplot(data=data, x='Score', y='pos', ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73018bc9-0bb7-44cc-9921-e15e0c4b98af",
   "metadata": {},
   "source": [
    "Here we have a negative relationship between score and negative columns. Negativity score decreases as score increases. This means that 5 star reviews have less negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87df15-e94a-4c92-8ddc-73405f75668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative sentiment score of each review\n",
    "ax = sns.barplot(data=data, x='Score', y='neg', ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eb179f-2f10-4542-91df-a7c7916cc781",
   "metadata": {},
   "source": [
    "Or you could be fancy and do them all together in one plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7b07ce-d014-4dc9-8213-44907c876ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n",
    "sns.barplot(data=data, x='Score', y='pos', ax=axs[0], ci=None)\n",
    "sns.barplot(data=data, x='Score', y='neg', ax=axs[1], ci=None)\n",
    "axs[0].set_title('Positive')\n",
    "axs[1].set_title('Negative')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a345f7-0033-49fd-b386-ba60ead0eb49",
   "metadata": {},
   "source": [
    "## Example 2: TextBlob. A rules-based approach to scoring sentiment\n",
    "\n",
    "TextBlob is a library built on top of nltk. It provides some additional functionality such as rules-based sentiment scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3132d7-be12-4998-828f-1199d83d1514",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbbcf8-8dc1-481d-91f1-52af2ebb89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "example1 = TextBlob(\"I love winter\").sentiment\n",
    "\n",
    "#again, polarity is measured between -1 and 1\n",
    "#subjectivity is measured between 0 and 1. This is a measure of how opinionated something is. \n",
    "example1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbe4121-dea0-4b61-b918-5c972a668458",
   "metadata": {},
   "source": [
    "### More about this module\n",
    "\n",
    "Linguist [Tom De Smedt](https://scholar.google.com/citations?user=8VBuRDwAAAAJ&hl=cs) has manually labeled all words in the english language ([from WordNet](https://wordnet.princeton.edu/)) their sentiment as \"positive\", \"negative\", etc. Let's take the word 'great' as an example.  \n",
    "\n",
    "![Great lexicon](./images/great.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27550867-0d27-4a46-bef1-f7a853a73e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob(\"great\").sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73986d4a-637a-47c8-900e-bdb2b5f7055c",
   "metadata": {},
   "source": [
    "Because \"great\" has several meanings, how do we know which one to use and which polarity/subjectivity score to assign to this word?  TextBlob gets the results above by just averaging all the polarity and subjectivity scores of the potential uses of \"great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeae036-4cdd-4401-9513-e144256915b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob(\"not great\").sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0101e-e215-4fd1-b94d-ba3fa56d4f69",
   "metadata": {},
   "source": [
    "\"Not great\" has a polarity score of -0.4, while the subjectivity remains unchanged. In this case, when TextBlob sees 'not' in front of something, it multiplies the polarity score of that word by -0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f225c-ac40-4bc7-acbe-394a30d8b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob(\"very great\").sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce4fefe-88b0-4921-95b7-9bff5236d4e2",
   "metadata": {},
   "source": [
    "If a word is preceeded by \"very\", both the sentiment and subjectivity scores are multiplied by 1.3, with a cap score of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d3bdf5-e49f-4e93-bf92-7a96fd540e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob(\"I am great.\").sentiment) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b3af8-7ff3-47ff-ac47-8c7001fc7c2a",
   "metadata": {},
   "source": [
    "\"I am great\" has the same score as our first example, because \"I\" and \"am\" do not affect \"great\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48bd310-6c14-470d-9dcb-629d914a3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TextBlob(\"I am great!\").sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824c048-4659-4c2c-a4ac-b97317649d6f",
   "metadata": {},
   "source": [
    "Punctuation also affects the scores. Here you see an \"!\" increases the polarity (though I don't know by how much)\"\n",
    "\n",
    "### TextBlob Summary\n",
    "TextBlob finds all of the words and phrases that it can assign a polarity and subjectivity to and averages them all together to get final scores. \n",
    "\n",
    "### Example with real text\n",
    "For this example, we are going to use TextBlob to analyze the sentiment of the Harry Potter book series. I found the text of all 7 Harry Potter books in [this github repo](https://github.com/formcept/whiteboard/tree/master/nbviewer/notebooks/data/harrypotter). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11e47a0-67f0-4341-ba3a-aa49b3997f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in text of Harry Potter: The Sorcerer's Stone\n",
    "harry_potter1 = open('../SentimentAnalysis_NLP-main/transcripts/HarryPotterPhilosophersStone.txt','r').read()\n",
    "harry_potter2 = open('../SentimentAnalysis_NLP-main/transcripts/HarryPotterChamberOfSecrets.txt','r').read()\n",
    "harry_potter3 = open('../SentimentAnalysis_NLP-main/transcripts/HarryPotterPrisonerOfAzkaban.txt','r').read()\n",
    "harry_potter4 = open('../SentimentAnalysis_NLP-main/transcripts/HarryPotterGobletOfFire.txt','r').read()\n",
    "harry_potter5 = open('../SentimentAnalysis_NLP-main/transcripts/HarryPotterOrderOfThePhoenix.txt','r').read()\n",
    "harry_potter6 = open('../SentimentAnalysis_NLP-main/transcripts/HarryPotterHalfBloodPrince.txt','r').read()\n",
    "harry_potter7 = open('../SentimentAnalysis_NLP-main/transcripts/HarryPotterDeathlyHallows.txt','r').read()\n",
    "\n",
    "book_texts = [harry_potter1, harry_potter2, harry_potter3, harry_potter4, harry_potter5, harry_potter6, harry_potter7]\n",
    "\n",
    "book_names = [\"Sorceror's Stone\", \"Chamber of Secrets\", \"Prisoner of Azkaban\", \"Goblet of Fire\", \"Order of the Phoenix\", \"Half Blood Prince\", \"Deathly Hallows\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541d31bb-a0f2-47be-b150-c8a27b96188b",
   "metadata": {},
   "source": [
    "Here I am creating a dictionary of the name and text of each book, then converting that to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920e25c-6603-4ce9-92e6-fd21f890a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = {}\n",
    "\n",
    "for i, book in enumerate(book_names):\n",
    "    book_data[book] = book_texts[i]\n",
    "    \n",
    "# create pandas dataframe with this data\n",
    "data_df = pd.DataFrame(book_data.items(), columns=['BookName', 'BookText'])\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c476bf88-edf6-4093-887d-cc35c1aba5ee",
   "metadata": {},
   "source": [
    "### Cleaning the data\n",
    "\n",
    "As we did earlier, there is usually some steps of pre-processing the data before we can perform sentiment analysis on it. We are only going to do one round of data cleaning for the purposes of this workshop.\n",
    "\n",
    "Common data cleaning steps on all text:\n",
    "\n",
    "- Make text all lower case\n",
    "- Remove punctuation\n",
    "- Remove numerical values\n",
    "- Remove common non-sensical text (/n)\n",
    "- Tokenize text\n",
    "- Remove stop words\n",
    "\n",
    "<b> lambda functions: </b> small anonymous functions, meaning functions that are not named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca9725-4b78-479c-bcf6-cdb389527cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a first round of text cleaning techniques\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)\n",
    "\n",
    "harry_potter_df = pd.DataFrame(data_df.BookText.apply(round1))\n",
    "\n",
    "harry_potter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61423ac1-6bb7-4742-983f-29bd575ecf38",
   "metadata": {},
   "source": [
    "## Document-Term Matrix\n",
    "For many Natural Language Processing techniques, the text must be tokenized, meaning broken down into smaller pieces. The most common tokenization technique is to break down text into words. We can do this using scikit-learn's CountVectorizer, where every row will represent a different document and every column will represent a different word.\n",
    "\n",
    "In addition, with CountVectorizer, we can remove stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f62819-f257-448f-bb99-d48ba2ec1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a document-term matrix using CountVectorizer and exclude common English stop words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')\n",
    "data_cv = cv.fit_transform(harry_potter_df['BookText'])\n",
    "harry_potter_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "harry_potter_dtm.index = [\"Sorceror's Stone\", \"Chamber of Secrets\", \"Prisoner of Azkaban\", \"Goblet of Fire\", \"Order of the Phoenix\", \"Half Blood Prince\", \"Deathly Hallows\"]\n",
    "harry_potter_dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d139a-c501-48ff-b875-ffa58cdaece6",
   "metadata": {},
   "source": [
    "## Sentiment of Routine\n",
    "\n",
    "We could look at the overall sentiment of each book (and we will), but let's do something a little more interesting. In most stories, there is an arc to the plot. Stories alternate between positive and negative events and usually end up in a positive outcome in the end. Is this true of the Harry Potter books?\n",
    "\n",
    "Start with getting the overall positivity and polarity for each book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73ac1b-e561-4feb-b84c-2032876971a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply a lambda function to find the polarity and subjectivity of each story\n",
    "pol = lambda x: TextBlob(x).sentiment.polarity\n",
    "sub = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "harry_potter_df['polarity'] = harry_potter_df['BookText'].apply(pol)\n",
    "harry_potter_df['subjectivity'] = harry_potter_df['BookText'].apply(sub)\n",
    "\n",
    "harry_potter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c009274-e357-4ea4-820b-4134a6576037",
   "metadata": {},
   "source": [
    "## Sentiment of Routine over Time\n",
    "\n",
    "Most Storylines have an arc. What is the arc of the Harry Potter books?\n",
    "To get this (arbitrarily), we will split the text into 10 chunks and we will get the polarity and subjectivity score for each section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8059cd9-b59a-4aa7-aa37-2121e112ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def split_text(text, n=10):\n",
    "    '''Takes in a string of text and splits into n equal parts, with a default of 10 equal parts.'''\n",
    "\n",
    "    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n",
    "    length = len(text)\n",
    "    size = math.floor(length / n)\n",
    "    start = np.arange(0, length, size)\n",
    "    \n",
    "    # Pull out equally sized pieces of text and put it into a list\n",
    "    split_list = []\n",
    "    for piece in range(n):\n",
    "        split_list.append(text[start[piece]:start[piece]+size])\n",
    "    return split_list\n",
    "\n",
    "# Let's create a list to hold all of the pieces of text\n",
    "list_pieces = []\n",
    "for t in harry_potter_df.BookText:\n",
    "    split = split_text(t)\n",
    "    list_pieces.append(split)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253900a5-1702-4b01-8465-4887aea9e17e",
   "metadata": {},
   "source": [
    "To demonstrate this point, first this shows there are indeed 7 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd8f9b-051a-4c26-8e92-2df51f0b8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_pieces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36234fcc-d57e-4599-b335-871df82ab52b",
   "metadata": {},
   "source": [
    "And now that there are 10 chunks in each book's text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d85a4-4104-462d-a371-20888a3b1875",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_pieces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f3748-5e47-4b70-bb3b-efefba838203",
   "metadata": {},
   "source": [
    "Calculate the polarity score for each section of text in each book. In the end there should be 70 scores. 7 books x 10 sections per book = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9f599-88a6-462b-8e8d-4f3244356ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the polarity for each piece of text in each book\n",
    "\n",
    "polarity_transcript = []\n",
    "for lp in list_pieces:\n",
    "    polarity_piece = []\n",
    "    for p in lp:\n",
    "        polarity_piece.append(TextBlob(p).sentiment.polarity)\n",
    "    polarity_transcript.append(polarity_piece)\n",
    "    \n",
    "polarity_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44c20c-378d-4c7b-b032-777d121b4b55",
   "metadata": {},
   "source": [
    "Show the polarity of sections in the first book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b584f-8560-4cfe-bf79-8c87a74508b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the plot for the first book: Harry Potter and the Sorceror's Stone\n",
    "harry_potter_df['BookTitle'] = [\"Sorceror's Stone\", \"Chamber of Secrets\", \"Prisoner of Azkaban\", \"Goblet of Fire\", \"Order of the Phoenix\", \"Half Blood Prince\", \"Deathly Hallows\"]\n",
    "\n",
    "plt.plot(polarity_transcript[0])\n",
    "plt.title(harry_potter_df['BookTitle'][0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3742743-0d70-4b84-bbac-c79c7eb518c8",
   "metadata": {},
   "source": [
    "Lastly, show the polarity scores for all 7 books in one plot. This will show the 'arc' of each book. One thing that caught my eye is that the 6th book (Half Blood Prince) ends on a negative note while the 7th book (Deathly Hallows) ends on a very positi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e42da4-6f41-41cf-a120-efd302a6a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all books in one plot\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [16, 12]\n",
    "\n",
    "for index, book in enumerate(harry_potter_df.index):\n",
    "    plt.subplot(3, 4, index+1)                             # gives each book its own plot                  \n",
    "    plt.plot(polarity_transcript[index])                   # plotting polarity score of each book  \n",
    "    plt.xlabel('Book Segment')                             # x axis label\n",
    "    plt.ylabel('Polarity')                                 # y axis label\n",
    "    plt.xticks(np.arange(0, 10, 1.0))                      # adds extra ticks on x axis\n",
    "    plt.title(harry_potter_df['BookTitle'][index])         # title of each plot\n",
    "    plt.tight_layout()                                     # spaces out plots\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45ab60-f96d-439c-8ac2-07c35fe14f93",
   "metadata": {},
   "source": [
    "## Sources / Self Help\n",
    "\n",
    "I like to end every workshop with a self-help section and where to go for more help. As always, this is just an introduction to these topics.\n",
    "\n",
    "First, here are a couple resources I used to put this video together\n",
    "\n",
    "- [pyOhio Natural Language Processing Workshop](https://www.youtube.com/watch?v=xvqsFTUsOmc)\n",
    "\n",
    "- [Sentiment Analysis in Python](https://www.youtube.com/watch?v=QpzMWQvxXWk)\n",
    "\n",
    "### UVA Resources\n",
    "Remember, you can always reach our to me or others here at UVA for more help! \n",
    "\n",
    "[UVA Statlab](https://data.library.virginia.edu/statlab/)\n",
    "\n",
    "[UVA Research Computing](https://www.rc.virginia.edu/)\n",
    "\n",
    "[UVA Digital Humanities](https://dh.library.virginia.edu/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c41b105-7bc7-41bf-ab17-038fdd9b3123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
